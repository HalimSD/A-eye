hi there uh so a couple of days ago openai uh published two really exciting
uh research works and uh among them dali took pretty much all of the media
attention and was all the hype but i i found this work uh clip uh or learning transfer
transferable visual models from natural language supervision uh really exciting uh and i'm i want to
cover it in this video so basically uh the the method itself is not new so they it's just a modified
version of of convert paper uh but the the thing there they introduced here is a concept of
using zero shot in computer vision so they're they're probably not the first ones but they are really bullish
on it uh they're just is pretty much a continuation of their exploration of the gpt
and all of the zero shot capabilities it had and they're now trying to take that nlp
paradigm and bring it back into the computer vision so uh yeah so this paper is mostly about
how this model can how can we create models which are which mimic the human brain
and i'll learn the way we do so with that being said uh let me jump into the method and uh i'll first explain the method and
then we'll we'll go through the different experiments uh they did okay so they say here we demonstrate
that the simple pre-training task of predicting which caption goes with which image is an efficient and
scalable way to learn soda image representations from scratch on a data set of 400 million
image sex pairs collected from the internet and so they they also introduced this new uh data set uh because all of so so
because they they really so that's open ai they really are bullish on computation and uh all of
the existing data sets uh weren't leveraging the the scale as much as they could
and especially since uh most of the computer vision tasks are uh benchmarks uh data sets like uh like
imagenet are only associated with a simple uh like uh structured
label and they just want to associate images and text so they had to create
their own data set so without further ado let's let's jump into the method itself and i'll i'll explain it in in depth
i'll start doing some high level explanation i will slowly dig deeper into the method
okay so this is this is how it looks like uh basically what they do is the
following so they have as i said associated with they have associated image text pairs and the way the the thing
they do is they first do some data augmentations here for the for the image part
and then they have image encoder and they have text encoder and here they just used uh
resnets and the newly introduced vision transformer whereas for the text encoder they just
used the original vaslani transformer with certain modifications they did for the gpt
family of models as well and uh once you once you have that set up what you do is the following
so you you take the text sequence and you just uh encode it and how do you do that well
because this is transformer so you basically just embed the text sequence here and you just
uh take the start of sentence token and you take the end of sentence tokens
and you place them you just bracket the text sequence using those and now
uh if you know how the transformer works and i hope you do so you just have the forward pass through the transformer
and whatever comes out from the last layer of the transformer above the eos
end of sentence token that's the representation they're going to use so they just do some simple layer
normalization here and they project this using some linear projection layer into the embedding
space and we're here so 1 through n is just n is just the size of the batch
so these are the encodings we get for the text uh sentences sequences
on the other hand uh image encoder is pretty uh straightforward you basically feed through the the image
and you get a certain representation at the output so if this was resonant you'd probably
just take the output at the after the average pooling part uh whereas they did some modifications with
resonant i won't get into those details right now but yeah you can imagine you just take out some a certain
representation at after some after that pulling part and you use that as your
uh image representation you also do that linear projection and you're finally inside this
contrastive embedding space so once we're here um we basically on
the high level we just want to make sure that this image image one
has the highest cosine similarity with this text embedding
vector t1 and you want to push all of these down to zero and you want to do the same thing along
this column so you want to make sure because we have asymmetric setting here we have on one side we have textual
information and on the other side we have visual information in our embedding space so this is a multi-modal
embedding space and so they do it both ways so they want to also make sure this one goes up to two to one
and all of these are pushed down to zero and you just repeat all of that across the batch
and that's it now um let's let's get one level deeper and see how this exactly
works so uh this paper was heavily inspired by uh this convert paper uh that was
working on medical images and they actually achieved some really awesome results uh so the the the visual features they
trained uh using their method uh proved to be really good at some downstream medical tasks
so uh you can see the the the how the pipeline looks like and it's the same thing as i just explained so
we have sequence of we have a sequence of images and sequence of associated
textual sequences and basically what they do is they do data augmentations which is
really really popular thing to do in these contrastive learning methods and they did some textual augmentations
so just extracting single sentences from their text sequences so this paper on the other hand ditched
this thing here because most of the images they had had only one single sentence associated
with image so they can just remove the sentence otherwise they'll they'll lose this supervision signal
so they just ditch this away and they simplify the this augmentation part where the only
thing they do is basically they resize the image and they take a
square crop that's everything no color jitter no blurring nothing so that's that's
everything now the second thing they did is so they modified as basically
this was mlp in the previous work and also so by the way this work this convert
paper uh was inspired by simclear framework which you probably heard of which
achieved really nice results on self-supervision visual representation learning
and it introduced this concept of using mlp between the embedding space and between the contrastive embedding space
so in this paper they just ditch this away and they just use a linear projection layer and that's it
so no activation functions you just have one matrix here that projects from the embedding space
into the contrast embedding space so that's one of the the other modification they did to the pipeline
um okay so once we have that let's let's see uh like some formulas for the things i
already intuitively explained how how it functions so uh so what we do is the cosine similarity
so this thing these uh like angle brackets just symbolize the coast and similarity
so once again you have two vectors let's say this is vi
and this thing here is ui and because this is a cosine similarity
that means these two vectors are normalized they are basically unit norm so the the norm is equal to
2 1. and so basically when you do the the the dot product you end up having cosine
of t so cosine of t where of theta where theta is the angle between
those two so again if the the theta is zero we'll have cosine similarly one if the
theta is zero that means they're orthogonal et cetera so when we say we want to make these two
uh be equal to one that means those uh vectors are pretty much aligned and
uh that's that's the idea so t here is the temperature coefficient just it just modifies the softmax distribution to
make it a bit more steep and it's actually trainable parameter in this paper and not a
hyperparam and this is a familiar soft mix thing so basically
what you're doing you're you're summing so you take the cosine this cosine similarly and you just average you so in the
denominator denominator you just sum up all of these and you get the probability distribution
and then you just have a simple cross entropy loss over that softmax distribution
which is a pretty usual thing to do so you can treat this as a classification problem basically you're trying to classify
which out of these combinations is the true combination so what's the true class and because as
i already mentioned this problem is asymmetric you're basically doing the same thing just this time you put ui
here and your your denominator part goes sums over the column and the final loss
vector is just a weighted combination of those two uh and you just go over the whole batch
you sum over the batch and you average out so that means you you take these
and then you also add this one to the loss and you add the row part
and you just do that for the whole batch so yeah that hopefully that was uh detailed
enough uh for you to understand the exact way this method works so so once you have the loss you just
backprop the information the gradients through your encoders and you train the image and the text
encoders and that's it your model is ready for this zero shot learning uh and
i'll i'll get to that in a second i just want to mention one more thing about uh data augmentations and the reason they
just took they just did this simple geometric augmentation uh whereby they just cropped uh from
from the resized image and not use uh color data so seem clear this framework i already mentioned
found this this nice finding so one composition of augmentation stands out
random cropping and random color jitter we conjecture that one serious issue
when using only random cropping as data augmentation is that most patches from an image
share a similar color distribution so that means that the the model can learn to cheat by
just looking at the histogram distribution and figure out that those are the same instances now
the difference between this framework and uh and clip model is that they they had they just compared
images to images in contrast to loss whereas we have now text and image so the reason i think they just stitched
away with with a color jitter is the efficiency and we'll see that uh in a couple of minutes
because uh yeah that's open ai they're just trying to scale things and and see how they uh how they perform
uh with that scale so what what you can see here is the following uh so this thing here is basically you
have an image and if you do know if you if you if you just teach the color jitting
editor and you just take a crop and you take another patch and you take another patch and you take another patch
so all of these patches will have as you can see here for four different patches they'll have a similar color distribution
and so once you do the contrast of learning between those augmented patches they'll it will
be really easy to even they they are visually different the histograms are the same so it will be really easy problem to just
place them in the same part of the embedding space where here once you do the color reader you can see that
the model can't explain okay cannot exploit this this this this this trick and that's the
reason they use color data but as i said probably because of the efficiency reasons so it was a trade-off
they they just used uh the random cropping okay that was it uh now let's focus on this
part and this is probably uh arguably the most important part of this paper and that's
how do they use uh this thing now so after they trained it how did they use it in a zero shot setting
and this is how so basically let's say you have you now wanna just uh test this one in a zero shot a
setting to a new you know on a new uh like downstream uh computer vision tasks like
let's say we have cipher 10 and that's a pretty famous and simple uh benchmark in computer
vision and you basically have 10 classes right and so maybe plane car dog bird etc so what you do is the
following you basically embed you embed those classes in sentences
of this construction maybe a photo of uh and then you just fill in the blank you put a plane your
car dog whatever so and once you do that
you just encode those sentences in the same way so we have end of sentence token blah blah this is
transformer but this time this one is trained so we have it initialized with certain weights uh after the training
after this contrast of learning training and so we just prepare all of those
embeddings so we just pre-compute them once and that's it we we're we are finished
with text encoder we just have uh this uh dispatch of embeddings here
and now how we do the zero shot uh classification is the following we just
take the image that we want to get classified we find its embedding vector and we just
do a simple cosine similarity between this embedding vector and between these embedding textual
embedding vectors and whatever the highest similarities that's our class and you can see here a photo of a dog
and because t3 corresponds to that particular sentence and it had the
highest similarity so maybe this one has and all of the others had maybe i know
0.1 this one had even smaller uh similarity etc so
that that's how the method works in a nutshell and yeah hopefully this was clear enough
and they also mentioned it here at test time the learned text encoder synthesizes our zero shot
linear classifier so synthesizes that's an interesting word because you can basically treat this as
a as a hyper network so you can basically treat this whole system with text encoder
and with these sentences as an adaptable as a hyper network so basically depending on the on these uh on your
data set you construct certain corpus of of sentences and those will adapt this text encoder
so that it's like a configurable linear classifier in a way let's call it that way so if you're
still having problems understanding why this is a a linear configurable classifier in a way let's look at it from a different
perspective so uh as i mentioned we have those linear projection layers from the
embedding space of the encoders into the contrasting embedding space so that means
after the embedding space here we'll have a linear projection which will project this vector into this
vector so that will end up having same dimensions across these image vectors in these text
vectors so that means this one is d dimensions and all of these have d dimensions
so let me take a look let me just extract this box and represent it as a vector which it is and these are also d
vectors so when we do when we do cosine similarity when we do dot product we're basically doing in a way where
we're creating a fully connected layer so these weights here these weights
correspond to maybe t1 and we'll end up with a value here and we do the same thing with t2
t2 is also a fully connected layer over this vector here and we end up with a second
value and we do that n times so we'll have n values here so that's
like n heads and you can see that all of these weights here are basically
configured depending on the text corpus because text encoder is pre-trained
and it's fixed and we'll just we just input these uh this text corpus that's created
out of the labels for that particular data set and we end up with particular weights here
which again are basically uh specifying the weights in this classifier and that's why it's a
configurable classifier and so that's really interesting so a zero shot linear classifier by embedding
the names or descriptions of target data set classes and that's we already saw that so we just embed it
so one detail i'll just i omitted here is that basically what it do here is
uh in assembling and prompt programming so if you're familiar with the gpt family of models you know that prop
programming is a thing that's just it's called like a software 3.0 and um the idea is depending on how you
construct these uh these these uh the skeleton around the class
you'll get different performance out of your model so what they ended up doing is they played with different constructions
and they also assembled bunch of these constructions and they just average out these the the
features uh here before they they project them into the contrasting embedding space so what that means maybe maybe they'll
have so for for car they maybe have a photo of a car but they'll have another sentence uh
like a nice photo of a car for example and they'll just take both of those embeddings and
they'll average it out before they project it into contrast space and they they found that that small trick
actually improves performance significantly but we'll get to that in in a bit more detail a bit later okay so
that was that was the explanation and now let's um uh let's dug it even a bit deeper and
then we'll we'll jump to experiments so um let's see
as i said we find the clip similar to the gpt family learns to perform a wide set of tasks
during pre-training including ocr geolocalization action recognition and many others so
again if you're familiar with gpt family of models you know that uh there is this emerging
uh let's call it phenomena happening so where you're basically training your model on
on some tasks like gpt3 was just trained to predict the next token and once you do that uh they they
figured out that it learned how to do many other tasks that it wasn't explicitly trained to do like maybe you
input a sentence into gpt and you just append too long
didn't read uh words here and then after you sample out
of it you'll notice that it actually learned to summarize the this input sentence even though it
wasn't trained to do that and the same thing happened here with clip they trained it to as we saw to
just associate the the most probable text sequence with the image and along the way it learned to do all
of these things so that's that's interesting okay i'll get to this pick in a second
let's just uh go over a couple more things so learning from natural language also has an important advantage over most
unsupervised blah blah so uh basically this says that uh the the the way how we train it enabled us to do
this flexible zero shot transfer as you saw with those uh with the hyper network
story i just explained you and um yeah i mentioned that they created this
wit data set which has 400 million pairs of images in text so why they did it is
because they want to leverage the scale and none of the existing data sets uh had that so that's why they did it and
um it's interesting how they created it so they had like a half a million queries which they created like by collecting uh
words that appear at least 100 times in english version of wikipedi wikipedia and some other heuristics and
they try to balance it out so that every one of those queries have has a
like a similar number of images associated with them that's how they created the data set
so but but more importantly uh let's jump into this part about efficiency so let's see why they ended up having the
exact same task that i just explained um so they say here that mahajan and his collaborators
folks from facebook required 19 gpu years to train this resnext
model uh she and others required 33 tp v3 core years to train the noisy
student so that's a lot of compute and the only thing they are trying to do is predict hundred imagenet classes
and compare that like comparing that to the uh to learning the open set of visual concepts from natural
language seems daunting so if this was so hard to do then just imagine
how much harder it is to train the the the model of uh predicting um like uh just natural
language uh labels uh that they're open form uh they did try uh first exploring with
this discriminating method that has a predictive objective instead of the uh contrastive objective i just showed
you and what vertex uh paper did is they basically also had
a image encoder and they had a transformer above it they were trying to predict the exact
same where it's from the caption of that image so you have an image you have a caption associated with it and you're trying to
predict the exact same word so that's not a new approach like we've been doing these uh captioning uh
uh predictions for for all for a long time and people noticed that the visual
representations you get out of these tasks so when you for example do some linear probing on these representations
that they are really useful for many downstream tasks classification etc so the thing is they they tried this one
and it was too expensive and we'll see the one of the charts above uh which will nicely explain this
and the second thing they also consider these generative methods like uh image gpt
uh what it does it learns to uh auto aggressively just uh regress the the the image
uh the input image and uh it it also learns really nice visual representations but the thing is it's
not efficient and like efficiency is the the the the key point here like this is openi uh they want to make
the most efficient method and then leverage the compute they have both in the harvard sense and the huge
data set they collected so like the bitter lessons by sudden uh are a nice read if you're not familiar
with this but basically the more compute you can throw at the problem uh usually that that's that's a good idea uh i'm not i'm
not i'm not saying that's the only way we can we will go forward with the research but that's definitely an option okay uh having said
that let's let's just jump to this chart and um what it did is the following so this
transformer language model that's basically that vertex approach i just mentioned so you just uh basically s
stick the the transformer on top of the image encoder and you can see that by trying to
predict the exact same words that from the caption it's the the the zero shot image net accuracy is really
low and you need a lot of data to accomplish it uh compare that to just a simple baseline where instead of trying to
predict the exact same words from the transformer you're just trying to predict
which words appear in your caption so that's a much simpler problem so you have a like a vocab here like maybe 30k of
words and you're just trying to predict which words are present in your current caption and
that's this uh orange line you can see and we we already noticed that we have a
3x efficiency gain and finally once we ditch the predictive objective
altogether so trying to predict the roots and just trying to associate the the textual as sequence with the
image we get a boost in efficiency a further boost and now we can truly leverage the
computation okay so that was that was it that was it about the efficiency part
and uh now i'll go into some more details about the method and then we'll we'll we'll just uh
jump to exploring the zero shot results they got so uh they say here that they we train
the clip from scratch without initializing the image encoder with imaging up weights or text encoder
with pre-trained weights and why they do that is because work
like a vision transformer already showed that once you have enough compute and enough
data you really don't need to bias your model in any way so you just
uh so what vision transformer showed is that this particular model is better than
cnn's given enough uh data like they use jft 300
data set and that's google's proprietary data set and they show that it outperforms cnns and it learns
something similar to cnns just a bit better obviously so that's why they don't encode it why they don't
initialize it with pre-drain weights so i explained most of these so i'll just skip them
um yeah this is an interesting detail they are not using so they for resnet they just swap this average
pulling layer with the attention pulling layer that's implemented in a transformer style and yeah um
they're using as i said was funny for the text encoder and i also mentioned this one about eos
so the interesting part about how they scaled their approach is that they used efficient nets
ideas so they used ideas from this paper to scale uh i don't know how to write
today okay to to scale their their technique and i just highlighted these as a as a just to show
you how much engineering uh goes into all of their all of their research work and like uh
you can see all of this mixed precision gradient check pointing is all about uh making use as
efficiently as possible of the memory available in there on their hardware so have precision have
precision and yeah it still takes a lot of time to train their biggest models so it took
18 days to train on almost 600 wii hundred gpus and 12 days on 256 for the
vision transformer which is a vision transformer is more efficient than cnns and that's one of the points that that paper made and
that's why it takes a lot less compute and finally the the best model
they had is the vision transformer l uh that was uh trained for one epoch
on on a higher resolution of 336 pixels uh and that's that's a final clip model
uh that they um that they are using throughout this paper okay finally experiments so as i
mentioned the target the goal of this paper is to show that clip has a really nice zero shot uh transfer performance and
what they mean by zero shot transfer performance is a bit more general than what you usually mean um by saying
that in computer vision so usually uh and and it's here they say here most of these tasks are uh are are
testing for distribution shift and domain generalization so what that means is that previous models in computer version basically you
want you want to test if it's if it's uh if it's having a really good performance
even when you change the this underlying distribution so basically here you have
a banana like a real world banana like a sketch and those are called different domains
and uh the other thing that we're testing for is that for example if the model was tested test uh was trained on real world
objects like banana i don't know dogs cats whatever and you want to see how how easy it is to just
do some fine tuning or do some linear probe on top of the feature space to maybe learn uh also about a new
object like maybe horse and what they mean by zero shot transfer learning in this
and this paper is not generalizing to those to those so distribution shifts and domain generalizations but to new
tasks so for example that they said here well it is reasonable to say that the svhn dataset which is this one
measures the task of street number transcription on the distribution of google street view photos
it is unclear what real task the cipher 10 data set measures and this is cipher 10. so uh basically
if you train the model maybe imagenet and then you you want to test it on on on cipher10 like this you're you're
basically doing what i already said so you're doing basically uh just a different different
uh distribute it just has a different distribution and maybe some different objects uh whereas if you're testing on svhn
you're basically learning how to do ocr in the wild if your model learns how to extract numbers uh from these images so that's it so we
we want to generalize not only to distribution shifts and generalizations to new objects we also want to
generalize to new tasks like ocr and not only to different classification data sets okay
let's let's start exploring the paper first this table is not so important
they just show so visual engrams are our method from 2016 which have a similar similar notion of zero shot
transfer and that's why they compare with them although it's not comparable because many things like transformers did not
exist back then and so yeah they just have much better clip just has much better results
on these data sets like imagenet so before seeing the zeroshot and fuchsia
performance of clip uh let's first go through prompt engineering and assembling details so the first thing they had to cope with
is the fact that there is a policeman problem in some of the downstream computer vision tasks and that means that for example in some
of the data sets like imagenet uh you basically have two words which mean two different things so different
semantics and maybe uh they give example here construction cranes and cranes the fly
so the first thing they had to do is for a given data set and labels to disambiguate between those labels
which have the polysemy problems the second thing and i already mentioned this one is they had to construct
uh the the sec the sequences like this so instead of just using labels so for
example if you have a on a downstream class task you have uh labeled dog instead of
using doc to to in your zero shot classifier to configure it you use something like this a photo of a
label and by doing that you get a 1.3 percent uh increase in accuracy and
that's like a low low hanging fruit the second thing they did is they they again they play with prom
programming so instead of just using a photo off a label they also experiment with adding maybe a
type of pet if that made sense for the so we found several fine-grained image classification data sets that it helped
to specify the category so for example oxford uh pets using that one uh would
even help them even boost the performance even higher so on ocr just putting the quotes around
the numbers would also gain uh gave them some additional a boost they also played with ensembling
as i mentioned so here is a concrete example so you by using a photo of a big for example dog and a photo of a small
dog and combining those in the in the embedding feature space uh they they get they get an additional
boost in performance so they did not average in the probability space they just average before
they do the the the softmax and everything so considering those two things together
they get a boost of almost five percent and that's on imagenet so that's that's a really uh
uh long low hanging fruit and they made use of it so um let's see that on the chart here
basically for a given compute by just using those additional
ensembling and pro program programming uh methods they just get a a huge uh improvements in the average
score so that's that's uh that's the first thing i wanted to mention now let's jump to the zero shot uh
of clip so what they did here is they took off the shelf model such as resonant
hundred i think no residence 50 here and they just fully s pre
uh fine-tuned it on every single data set you see here so it they pre-trained it on stl10
stanford cars etc and they took on the other side it took clip in a zero shot
setting and we can see that clip is much better than fully supervised
resnet resonant 50 on many of the data sets and now if we just analyze the extremes
here this part and this part that will probably give us some more information what's happening here so first things first the data set
zero shot clip improves by the most uh is stl10 a dataset designed to encourage
efficient learning by providing only a limited number of labeled examples so what that means is that for this
particular data set because resnet was uh you basically take the resonance features that were that we got
from imagenet and we freeze those and then we take the classifier and uh training that one uh with this
many with this uh like this small amount of examples uh just didn't didn't uh give us a good
results so you can see that clip was much better than uh resnet and on the other hand we have
this other part of spectrum where if we analyze these data sets you can see that
these datasets are complex and specialized so they say it here um looking where it
underperforms it works really weak on on these datasets like satellite image classification lymph node tumor counting
objects blah blah blah and these results highlight the poor capability of zero shot clip on more complex
complex tasks so that's where it underperforms uh resnet uh but it's still quite fascinating that
a fully supervised model using uh labeled information uh is under performing clip
on on many of the uh data sets so it's worth mentioning of course resonance 50 is by no means
uh state of the art currently so let's now examine um what happens when we compare
clip with state of the art but in a few shots few shot setting this time so these models are not trained
like uh with all of the labels they just take a couple of labels so um let's analyze the chart
and they took three models first is resonance 50 sim clear the self supervised uh
framework i mentioned and bit big transfer so this one was trained on jft 300 so
a lot of data and we can see that with 16 um so we just so what we do here is
we we take those models like bit and we freeze the weights we add the classifier
and we we pre tr we fine tune um on these different data sets
and we see that for 16 um examples we are approaching the zero
shot uh clip which is a nice nice nice result uh because these are the the best
methods available currently and what's interesting here is that there is this
drop between zero shot and one shot and basically that discrepancy
shouldn't be there in in an ideal world because humans for example when when you when you give us one example
we won't be underperforming like uh uh yeah we'll we'll we'll be better so uh
that's something they're trying to to to to work on and the potential solution they they propose is just to
uh initialize so once so you take the vision encoder of clip and instead of
uh training a classifier from random they just initialize it with the zero
classifier zero shot classifier as weights uh for that particular data set and use that but that still didn't help
them so they just left it out for for the future for the future exploration
let's continue and see how the how good the zero shot
classifier actually is and here we see what we see in this chart is
for example how many labeled uh examples do we need to achieve the same uh accuracy as
zero shot classifier so for for fair 2013 uh if we just
take that division encoder of clip and use a random initialized classifier
we need 184 examples to achieve the same performance as uh zero shot clip so that's a that's
a good result we we we want to make a zero shot classifier as good as possible so that even with a
lot of labeled data we can't improve upon it so that would be an ideal result if every single data
set required a lot of labeled examples to improve upon that
randomly initialized classifier that would be a good result here on the other chart we can see the
difference between a linear probe clip performance versus a zero shot
uh performance so this time uh you take that uh classifier and you basically
uh train it with all the available data so it's fully supervised and we we use
all of the data available in a particular data set and we see that for a given um data set
like maybe this one the the performance is maybe 45 once we
use all once we leverage all of the data but by using zero shot we have only 35. so obviously zero shot
a classifier is suboptimal for this data set and we can see the general trend so in a perfect world
zero shot would be uh as good as this classifier that used the whole corpus of label data and but
it's you can see the trend uh it's sub-optimal and it's down by maybe 20 or 15 um points and only a couple of
data sets like stl10 and if you remember that one has a little like a small amount of label data available so here the the
zero shot performance is similar to this fully uh fully trained uh classifier so
uh that shows us that uh there is a lot more like space to to improve upon this zero
shot classifier let's continue analyze other charts so this one
what it tells us is that um basically uh increasing the compute will decrease
the error which is something we we could expect but the interesting thing is it's really noisy so if we take a
particular data set we can see that with compute sometimes the error increase increases and then it decreases
so it's really noisy but the overall trend is still uh strong in in favor of more computation
is better here what we see is that basically
the feature space of clip is high quality because once we do once we compare it
with different models like you can see here efficient nets and build and moco
sim cleared like self-supervised methods and bit which is fully
supervised we can see that the feature space is higher quality for clip and so what we see on this diagram is
the amount of compute on x-axis and the average score across a suit of different like 27 data sets
and every single model um was had a had a linear probe a fully
fully trained uh on the frozen feature space and a clip again shows a much better
much better average score which is something that we ideally want we want the clip to to be
the features we train for clip are are really good for many downstream data
sets and it showed that uh here if we take one specific model like maybe efficientnet
a noisy student and we analyze we break down those instead of using the average
score we just see the performance across every single data set we can see where the
the results for clip are much better and where it's a bit worse than efficient net and because
efficiently was basically pre-trained on imagenet so it does make sense that it has better performance than clip that
the feature space he developed is better for imagenet but only for imagenet as we can see on most of the other data sets
the feature space developed by clip is way more high quality than than efficient nets uh feature space
okay let's continue and see some other results uh i'll skip this
one not that interesting and i'll jump to this one uh basically if we take uh what we see
here is the following we want to see we want to see uh how how the clip
uh accuracy behaves uh when we change the underlying
distribution so we see here basically same cl same object same classes but like different
distributions so here we have sketches here we have real world objects some like
again some drawings etc and if we take for example a model that had
80 accuracy on imagenet top one accuracy we see that um for all of those
uh standard imagenet models that they severely have like have much lower
accuracy on those other uh related data sets but just with different
distribution whereas on the other hand um if we take the results for uh
for clip it has much higher much higher accuracy uh than than than than imaging at once
and ideally again uh if we have 80 on imagenet we want to have uh 80
on all of those uh similar data sets just with different distributions
uh if we take a look at the tabular data here what we see is when we take resonance
101 and so it was just pre-trained on imogen we don't fine-tune it on any of
these new data sets with different distributions we just use it as it is and we see that the
performance that the accuracy just severely drops and for these adversarial examples like
you can see here some chopped banana something that the model never saw during training it has really
really bad accuracy on the other hand clip has got much much higher accuracy
and it improves like even 75 percent in in this case on this data set
so that shows that a clip is much more robust to distributional shift than
these other other methods let's see some other results they showed and this one is interesting
so this line is the same as the one above so we just use clip in a zero shot setting and if we actually um
fine fine-tuned clip for imagenet we can see that even though it improves
the image and accuracy uh the the the actual robustness to to
distributional shift so again we have those uh data sets like those seven data sets we see that
the curves actually go go down and it's it's less robust to to all of those uh data sets
from another perspective uh if we look at this chart uh it's it has some improvements on
imagenet whereas on many other uh data sets it just
has less less good performance so what's this adapt to class shift
so i think i noted somewhere here basically uh because some of these data sets like
youtube bb and imagenet bit have different classes than imagen which has thousand classes so
it has so for the person class in youtube bb we have what what we have to do for
those imagenet models is uh uh collect the scores over baseball player
uh bridegroom and scuba diver and that's how we give the prediction
for person on youtube bb which is sub-optimal but since clip
is configurable we can just use this new class and create that construction
a type of a person or whatever to to have much better results than than these
uh fixed image net models and when we do that we we have uh we can see that the trend
improves and the robustness increases further uh which is really nice and particularly on those couple of data
sets which has which have that super set of classes uh this uh small trick helps whereas the other
methods as i already explained have to do the following so for example this this data set and it has maybe
class person so when we're testing when we're testing efficientnet we have
to pull over three classes out of thousands that he has to give an estimate of what person would
be since he doesn't know of the concept of a person and on the other hand we can just use the
flexibility of a zero shot classifier and uh get the direct results
uh instead i'll just i'll just skip some of these because this is similar experiments to
these ones uh it's just that instead of doing uh showing us the fully supervised curve so
this model is clip that's uh that's fine tuned to imagenet
fully fine-tuned and instead of that they just show a trend once we start from few shots so two
shots four shot as we start increasing the number of labels from imagenet we can see that
um we are slowly increasing the the imagenet top one accuracy but the uh we're still underneath uh
zero shot uh clip uh on this on these seven natural distribution
data sets okay there's there's a lot of information here um the main idea is basically that
uh clip clips feature space is way better uh suited to to just extend to many
other downstream uh tasks and it's much more robust as we saw to different
uh natural shifts so in a way it's it's so it's a better way to and additionally it's not using those costly
supervised data sets it's just using those pairings um and so it's it's easier to to to just get a
get a get a grab of those of that data that's the main idea of the paper uh this shows some contamination
analysis and finally i just want to show that there are some examples where uh it's not working uh
really good and uh a nice nice example of a failure is an amnest
where it achieves only 88 percent accuracy on mnist just for
comparison if you take a simplest baseline so you just take mnist image which is 28 by 28 you just
flatten out the pixels and you use a simple fiford net
this one will achieve higher higher accuracy than clip which was trained on a lot of data and lots of compute so
it really failed on mnist so as they say here this suggests clip does little to
address the underlying problem of brittle generalization of deep learning models instead eclipse tries to
circumvent the problem and hopes that by training on such a large and varied data set that all data will
effectively be in distribution and this is a naive assumption that as amnes demonstrates is easy easy to
violate so obviously uh this is not the the final the final uh way to go
towards towards uh really generalized ma to really general models
uh but it's uh it's a it's a step forward and i'm excited about this work so yeah um that was that was pretty much
it a short recap because there was a lot of details and hopefully this will make it kind of consolidate in your head
and so uh they care about zero shot performance uh generalization to new tasks and not
only on new distributions and a domain generalization they showed that
prompt programming and assembling does help boost the performance of clip models so that's important
they showed that even against the fully supervised baseline such as the resonance 50 they are comparable and
those are exciting results they show that even against the best bass lines in a few shot setting they are
comparable in a zero shot setting and they are better when they're doing the same thing when they're
also fine-tuning their model linear probing actually here
next they show that zero shot classifier can be further improved it's sub-optimal to the
classifiers which can which are imaginable by just uh fine-tuning them on specific data sets so there is a lot
of there is there is more work that can be done on improving that uh again compute
doesn't does improve a clip uh features are better so feature space that clip
provides is better than other bass lines um and compared to a specific bass line
efficient net here we can see that it's much better on many data sets so again feature space is richer
than than the other baselines here it shows a impressive
generalization to a robustness to to distribution shift and uh same thing here and they show
that if you want if you if you kind of uh add a classifier and linear probe the the clip onto imagenet
you basically uh do improve image and performance but you lose the robustness to other data
sets so you don't want to overfit the imagenet basically and finally they showed some limitations
among them that it's failing on on on simple uh data sets such as mnist
because it never saw those images during training so it just fails miserably
even compared to the simplest of baselines okay that was uh there was a much longer video than i expected but hopefully you learned
something from this video uh if you did consider subscribing and sharing this video and also hit that
bell icon to get notified when i upload a new video until next time keep learning deep
[Music]